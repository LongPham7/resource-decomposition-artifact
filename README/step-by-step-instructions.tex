% !TeX root = README.tex

\section{Step by Step Instructions}
\label{sec:step-by-step-instructions}

\subsection{Instantiation 1}
\label{sec:step-by-step-instructions:instantiation-1}

\labelcref{introduction:instantiation:1} integrates type-based static resource
analysis AARA~\citep{Hoffmann2011a,Hoffmann2017} and Bayesian data-driven
resource analysis.
%
This instantiation is evaluated on the 13 benchmark programs listed in Table~1.

\paragraph{Benchmark programs}

In the artifact, the 13 benchmarks for \labelcref{introduction:instantiation:1}
go by the following names:
\begin{verbatim}
merge_sort, quicksort, bubble_sort, heap_sort, huffman_code,
unbalanced_binary_search_tree, balanced_binary_search_tree,
red_black_tree, avl_tree, splay_tree,
prim_algorithm, dijkstra_algorithm, bellman_ford_algorithm
\end{verbatim}
%
For each benchmark, the original OCaml program $P(x)$ is available in the file
\begin{verbatim}
/home/ocaml-benchmarks/lib/<benchmark-name>/<benchmark-name>.ml
\end{verbatim}
where \texttt{<benchmark-name>} is replaced the benchmark program's name (e.g.,
\texttt{merge\_sort}).
%
For the two benchmarks \unbalancedbst{} and \balancedbst{}, however, they share
the same OCaml source file:
\begin{verbatim}
/home/ocaml-benchmarks/lib/binary_search_tree/binary_search_tree.ml
\end{verbatim}

\paragraph{Static analysis}

To perform AARA~\citep{Hoffmann2011a,Hoffmann2017} (implemented in the tool
RaML~\citep{RaML}), go to this directory:
\begin{verbatim}
# cd /home/ocaml-benchmarks/
\end{verbatim}

If you wish to run RaML on an original OCaml program $P(x)$, run
\begin{verbatim}
# python3 raml/run_raml.py benchmark <benchmark-name> standard
\end{verbatim}
%
The inference result (and analysis time) is printed out on the screen and also
saved in the file
\begin{verbatim}
/home/ocaml-benchmarks/raml/output/<benchmark-name>/raml_output_standard.txt
\end{verbatim}
%
The results of running RaML on the original programs are shown in the fourth
column (titled ``AARA Baselines Basic'') of Table~1 in the paper.

The analysis time of RaML varies greatly from benchmark to benchmark.
%
For instance, \mergesort{}'s original program only takes less than
\qty{1.0}{\second}, while \redblacktree{}'s original program can take around
\qty{480}{\second}.

The resource-guarded program $P_{\mathrm{rg}}(x, r)$ of each benchmark is
available in the file
\begin{verbatim}
/home/ocaml-benchmarks/lib/<benchmark-name>/<benchmark-name>_counter.ml
\end{verbatim}
%
The resource-guarded program $P_{\mathrm{rg}}(x, r)$ is obtained from the
original program $P(x)$ by \emph{manually} extending the source code with
resource guards.
%
Our artifact does not \emph{automatically} perform this program transformation,
although it is possible to automate (as formulated in \S3.3 of the paper).

To run RaML on a resource-guarded program, run
\begin{verbatim}
# python3 raml/run_raml.py benchmark <benchmark-name> counter
\end{verbatim}
%
The inference result (and analysis time) is printed out on the screen and also
saved in the file
\begin{verbatim}
/home/ocaml-benchmarks/raml/output/<benchmark-name>/raml_output_counter.txt
\end{verbatim}
%
The analysis of RaML varies greatly from benchmark to benchmark: \mergesort{}'s
resource-guarded program takes less than \qty{1.0}{\second}, while \dijkstra{}'s
resource-guarded program can take around \qty{3180}{\second}.
%
RaML's analysis time for resource-guarded programs is reported in the last
column (titled ``Analysis Time Static'') of Table~1 in the paper.

To run RaML on both the original and resource-guarded programs of a benchmark,
run
\begin{verbatim}
# python3 raml/run_raml.py benchmark <benchmark-name> both
\end{verbatim}

Finally, to run RaML on (original and/or resource-guarded) programs of all
relevant benchmarks (i.e., the 13 benchmarks in
\labelcref{introduction:instantiation:1} and the benchmark \kruskal{} in
\labelcref{introduction:instantiation:2}), run
\begin{verbatim}
# python3 raml/run_raml.py all <mode>
\end{verbatim}
where $\texttt{<mode>} \in \set{\texttt{standard}, \texttt{counter},
    \texttt{both}}$ specifies which version of a program is fed to RaML for
every benchmark.

\paragraph{Dataset generation}

The resource-decomposed program $P_{\mathrm{rd}}(x)$ of each benchmark is
available in the file
\begin{verbatim}
/home/ocaml-benchmarks/lib/<benchmark-name>/<benchmark-name>_data_collection.ml
\end{verbatim}

Before performing Bayesian inference, we generate a dataset $\calD$ of resource
components' measurements.
%
To do so, first move to the directory
\begin{verbatim}
# cd /home/experiment/
\end{verbatim}
%
Then run
\begin{verbatim}
# python3 -m runtime_data_generation.runtime_data_generation benchmark <benchmark-name>
\end{verbatim}
%
The generated dataset $\calD$ (and also analysis time of Bayesian inference) is
stored in a JSON file
\begin{verbatim}
/home/experiment/bin/<benchmark-name>/bucket1/runtime_cost_data/runtime_cost_data.json
\end{verbatim}

If you wish to generate datasets for all relevant benchmarks (i.e., the 13
benchmarks in \labelcref{introduction:instantiation:1} and the benchmark
\quicksorttiml{} in \labelcref{introduction:instantiation:3}), run
\begin{verbatim}
# python3 -m runtime_data_generation.runtime_data_generation all
\end{verbatim}

\paragraph{Bayesian inference}

We perform Bayesian inference on the observed data $\calD$ to infer a posterior
distribution of recursion-depth symbolic bounds.
%
This is achieved by running
\begin{verbatim}
# python3 -m run_model benchmark <benchmark-name>
\end{verbatim}
%
This command invokes an inference engine of the Stan probabilistic programming
language~\citep{Carpenter2017}, drawing 44,000 posterior samples of inferred
recursion-depth bounds.

The analysis time of Bayesian inference should be less than \qty{3}{\minute} in
all benchmarks (and less than \qty{1}{\minute} in most benchmarks), assuming
that it is not the first time the command is executed.
%
Otherwise, the command can take longer when it is first executed in order to
build efficient machine code for a sampling algorithm.

To perform Bayesian inference on all relevant benchmarks (i.e., the 13
benchmarks in \labelcref{introduction:instantiation:1} and the benchmark
\quicksorttiml{} in \labelcref{introduction:instantiation:3}), run
\begin{verbatim}
# python3 -m run_model all
\end{verbatim}

\paragraph{Table of experiment results}

To display the analysis time of Bayesian analysis of all relevant benchmarks
(i.e., the 13 benchmarks in \labelcref{introduction:instantiation:1} and the
benchmark \quicksorttiml{} in \labelcref{introduction:instantiation:3}), run the
following command in the directory \texttt{/home/experiment/}:
\begin{verbatim}
# python3 -m run_analysis time
\end{verbatim}
%
This analysis time is reported in the second-to-last column (titled ``Analysis
Time Data-Driven'') of Table~1 in the paper.

To calculate and display the percentages of sound resource-component bounds in
all relevant benchmarks, run
\begin{verbatim}
# python3 -m run_analysis soundness all
\end{verbatim}
%
This table corresponds to the third-to-last and fourth-to-last columns (titled
``Sound Bounds Asymptotics and Coefficients'') in Table~1.

\paragraph{Plots of inferred bounds}

To produce plots of inferred bounds (e.g., Figures~3 and 4 in the paper), in the
directory \texttt{/home/experiment/}, run
\begin{verbatim}
# python3 -m visualization.visualization plot <mode>
\end{verbatim}
where $\texttt{<mode>} \in \set{\texttt{all}, \texttt{top}, \texttt{right},
    \texttt{no\_legend}}$ specifies where the legend is placed inside a plot.

The generated PDF files are stored in the directory
\begin{verbatim}
/home/experiment/images/<benchmark-name>/bucket1
\end{verbatim}
%
For instance, in the directory \texttt{/home/experiment/images/merge\_sort/bucket1}
for the benchmark \mergesort{}, we have two plots:
\begin{verbatim}
posterior_distributions.pdf, posterior_distributions_no_legend.pdf
\end{verbatim}

To transfer plots from the Docker container to your local machine, run the
following command in your local machine's terminal:
\begin{verbatim}
$ docker cp resource-decomposition:<source-path> <destination-path>
\end{verbatim}

\subsection{Instantiation 2}
\label{sec:step-by-step-instructions:instantiation-2}

\labelcref{introduction:instantiation:2} integrates static resource analysis
(AARA~\citep{Hoffmann2011a,Hoffmann2017}) and interactive resource analysis
(Iris with time credits~\citep{Chargueraud2019}).
%
This instantiation is evaluated on Kruskal's algorithm for minimum spanning
trees.
%
The benchmark \kruskal{} is implemented in OCaml and uses a union-find data
structure.
%
Given an input weighted graph, \kruskal{} first runs \mergesort{} to sort the
edges in the ascending order of their weights.
%
Initially, each vertex in the graph forms its own (singleton) spanning tree.
%
\kruskal{} then examines each edge in their sorted order, incrementally building
a spanning tree by merging two disjoint sets whenever they are connected by an
edge.

\paragraph{Static analysis}

AARA is used to infer an overall cost bound of \kruskal{}'s resource-guarded
program.
%
To run AARA, go to the directory \texttt{/home/ocaml-benchmarks/} and run
\begin{verbatim}
# python3 raml/run_raml.py benchmark kruskal_algorithm counter
\end{verbatim}
%
It returns an inference result:
\begin{Verbatim}[fontsize=\footnotesize]
== kruskal_algorithm :

  [(int * (int * float) list) list;
  ('a * 'b list) * ('c * int list)] ->
  (int * int * float) list

  Non-zero annotations of the argument:
  3.5  <--  ([::(*, [::(*, *)])], ((*, []), (*, [::(*)])))
  1  <--  ([::(*, [::(*, *)])], ((*, []), (*, [])))
  3.5  <--  ([::(*, [])], ((*, []), (*, [])))
  1.5  <--  ([], ((*, [::(*)]), (*, [])))
  7  <--  ([], ((*, []), (*, [])))

  Non-zero annotations of result:

  Simplified bound:
  7 + 1*L*M + 3.5*L*M*R + 3.5*M + 1.5*Y
  where
  R is the number of ::-nodes of the 2nd component of the 2nd component of the 2nd ...
  Y is the number of ::-nodes of the 2nd component of the 1st component of the 2nd ...
  M is the number of ::-nodes of the 1st component of the argument
  L is the maximal number of ::-nodes of the 2nd component in the ::-nodes of the 1st ...

--
  Mode:          upper
  Metric:        ticks
  Degree:        3
  Run time:      16.02 seconds
  #Constraints:  57606

====
\end{Verbatim}
%
The inferred bound is translated to
\begin{equation}
  f (d, \abs{V}, r_1, r_2) = 7 + 3.5 \abs{V} + d \cdot \abs{V} + 3.5 d \cdot \abs{V} \cdot r_1 + 1.5 r_2,
\end{equation}
which is parametric in the maximum degree $d$, the number of vertices $\abs{V}$,
and two resource guards (i.e., $r_1$ and $r_2$).
%
This inferred bound is reported in Eq.~(5.3) in the paper.
%
The analysis time of RaML should be less than \qty{20}{\second}.

\paragraph{Plot of inferred bounds}

The plot of the inferred bound in \kruskal{} (i.e., Figure~5 in the paper) can
be produced by moving to the directory \texttt{/home/experiment} and running
\begin{verbatim}
# python3 -m visualization.visualization interactive_analysis
\end{verbatim}
%
The generated plot is stored in the PDF file
\begin{verbatim}
/home/experiment/images/kruskal_algorithm/bucket1/interactive_analysis_result.pdf
\end{verbatim}

\subsection{Instantiation 3}
\label{sec:step-by-step-instructions:instantiation-3}

\labelcref{introduction:instantiation:3} integrates SMT-based semi-automatic
resource analysis (TiML~\citep{WangWC17}) and Bayesian data-driven resource
analysis.
%
This instantiation is evaluated on the benchmark \quicksorttiml{}, where the
comparison function used inside quicksort has logarithmic cost in the maximum
input natural numbers.

\paragraph{SMT-based semi-automatic analysis}

In TiML~\citep{WangWC17}, a user manually annotates the source code of a
Standard-ML program with a candidate cost bound.
%
The bound is then automatically verified by an SMT solver.

To view the annotated Standard-ML program of \quicksorttiml{}, go to this
directory:
\begin{verbatim}
# cd /home/timl
\end{verbatim}
%
Then run
\begin{verbatim}
# cat examples/qsort-rd.timl
\end{verbatim}
%
In the source file, the function \texttt{quicksort} is annotated with a candidate cost bound:
\begin{verbatim}
idx T_quicksort = fn m n => $n * ($n + 1.0) * ($m + 2.0) + 2.0 * $n
\end{verbatim}
%
This bound is also displayed in Eq~(6.1) in the paper.

To verify the candidate bound by TiML, run
\begin{verbatim}
# ./main.sh -l examples/basic-rd.timl examples/qsort-rd.timl
\end{verbatim}
%
Its output reports that the typechecking is successful:
\begin{verbatim}
...
Typechecking examples/qsort-rd.timl succeeded.
...
\end{verbatim}

\paragraph{Data generation}

To generate a dataset of resource-component measurements in \quicksorttiml{},
go to this directory:
\begin{verbatim}
# cd /home/experiment
\end{verbatim}
%
Then run
\begin{verbatim}
# python3 -m runtime_data_generation.runtime_data_generation benchmark quicksort_timl
\end{verbatim}
%
The generated dataset is stored in the JSON file
\begin{verbatim}
/home/experiment/bin/quicksort_timl/bucket1/runtime_cost_data/runtime_cost_data.json
\end{verbatim}

\paragraph{Bayesian inference}

We perform Bayesian inference to infer a bound of the logarithmic comparison cost in \quicksorttiml{}.
%
In the directory \texttt{/home/experiment}, run
\begin{verbatim}
# python3 -m run_model benchmark quicksort_timl
\end{verbatim}
%
This command can take around \qty{150}{\second} to run.
%
Posterior samples are stored in the file
\begin{verbatim}
/home/experiment/bin/quicksort_timl/bucket1/counter0/model0/inference_result.json
\end{verbatim}

\paragraph{Display evaluation results}

To display the percentage of sound resource-component bounds in
\quicksorttiml{}, run
\begin{verbatim}
# python3 -m run_analysis soundness all
\end{verbatim}
%
In the table printed out, the soundness percentage of \quicksorttiml{} should be
70.1\%, which is reported in L1033 in the paper.

To generate a plot of inferred bounds (Figure~6), run
\begin{verbatim}
# python3 -m visualization.visualization quicksort_timl
\end{verbatim}
%
The generated plot is stored in the PDf file
\begin{verbatim}
/home/experiment/images/quicksort_timl/bucket1/posterior_distributions.pdf
\end{verbatim}
