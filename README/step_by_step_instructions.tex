% !TeX root = README.tex

\section{Step-by-Step Instructions}

\subsection{Benchmark Suite}

The artifact contains 10 benchmark programs for Hybrid AARA as listed in
Section~7 of the paper.
%
These benchmarks are: \texttt{MapAppend}, \texttt{Concat},
\texttt{InsertionSort2}, \texttt{QuickSort}, \texttt{QuickSelect},
\texttt{MedianOfMedians}, \texttt{ZAlgorithm}, \texttt{BubbleSort},
\texttt{Round}, and \texttt{EvenOddTail}.
%
Inside the artifact, the benchmark \texttt{EvenOddTail} goes by its full name,
\texttt{EvenSplitOddTail}, instead of its shorthand \texttt{EvenOddTail} used
in the paper.

The paper evaluates both purely data-driven resource analysis and hybrid
resource analysis on the first seven benchmarks, and only data-driven resource
analysis on the remaining three benchmarks.

Inside the benchmark suite directory \texttt{/home/hybrid\_aara/benchmark\_suite},
each benchmark has its own directory, which contains the following three
subdirectories:
\begin{enumerate}
  \item \texttt{utility}: Python source files that contain (i)
        benchmark-specific configurations for Hybrid AARA and (ii) functions for
        analyzing the inference results.
  \item \texttt{bin}: input and output files for Hybrid RaML.
  \item \texttt{images}: plots of inferred cost bounds.
\end{enumerate}

\subsection{Display Experimental Results}
\label{sec:display experiment results}

In this section, we demonstrate how to produce tables and plots in the paper
from the experimental results that are stored in each benchmark's \texttt{bin}
directory.
%
In the Docker container, the \texttt{bin} directory of each benchmark already
contains the experimental results that we reported in the paper.
%
We will demonstrate how to generate the experimental results from scratch in
\cref{sec:generate experiment results}.

Go to the directory \texttt{/home/hybrid\_aara/benchmark\_suite/toolbox}.
%
Then run
\begin{verbatim}
# python3 run_analysis.py <table-or-plot-type>
\end{verbatim}
where \texttt{<table-or-plot-type>} specifies the type of tables or plots to
produce.

\paragraph{Soundness proportions}

In Table~1 in the paper, the 4\textsuperscript{th} and 5\textsuperscript{th}
columns display the percentages of sound cost bounds inferred by data-driven and
hybrid resource analyses, respectively.
%
To display these two columns, run
\begin{verbatim}
# python3 run_analysis.py soundness
\end{verbatim}

This table supports the two key claims in our paper (Section~7).
%
Firstly, in both data-driven and hybrid resource analysis, \BayesWC{} and
\BayesPC{} have higher percentages of sound cost bounds than \Opt{}.
%
Secondly, hybrid resource analysis (Hybrid \BayesWC{} and Hybrid \BayesPC{}) has
higher percentages of sound cost bounds than purely data-driven resource
analysis (Data-Driven \BayesWC{} and Data-Driven \BayesPC{}).

\paragraph{Analysis time}

The 6\textsuperscript{th} and 7\textsuperscript{th} columns of Table~1 display
the analysis time of data-driven hybrid resource analyses, respectively.
To display these two columns, run
\begin{verbatim}
# python3 run_analysis.py time
\end{verbatim}

The table of analysis time produced by the artifact looks different from the one
in the submitted version of the paper, in terms of both the numbers and layout.
%
This is because, since the submission, we have changed the hyperparameters of
Bayesian inference and rerun experiments.
%
In the camera-ready version of the paper, we will update the analysis time in
Table~1.

\paragraph{Relative errors}

Figure~5 in the paper displays relative errors of 5 selected benchmarks.
%
To produce this plot, run
\begin{verbatim}
# python3 run_analysis.py plot_relative_errors
\end{verbatim}
%
This command produces a plot and stores it as
\texttt{relative\_errors.pdf} inside the directory
\texttt{/home/hybrid\_aara/benchmark\_suite/images}.
%
To view the plot, we first transfer it from the Docker container to your local
filesystem.
%
Run the following command in your local machine's terminal:
\begin{verbatim}
$ docker cp hybrid_aara:/home/hybrid_aara/benchmark_suite\
/images/relative_errors.pdf \
<path-to-local-directory>
\end{verbatim}
where the second argument \texttt{<path-to-local-directory>} is a desired
destination directory on your local filesystem.
%
You can then view the plot on your local machine.

For Tables~2--11 in the paper, to create and display them on the stdout, run
\begin{verbatim}
# python3 run_analysis.py relative_errors
\end{verbatim}

\paragraph{Plots of inferred cost bounds}

To produce Figures~6--24 in the paper, run
\begin{verbatim}
# python3 run_analysis.py plot
\end{verbatim}
%
This command creates all plots of inference results and stores them in the
directory \texttt{benchmark\_suite/<benchmark-name>/images}.
%
It should terminate in 4 minutes.

For example, for Hybrid \BayesWC{} performed on the benchmark
\texttt{QuickSort}, its plot is the file \texttt{inferred\_cost\_bound.pdf} in
the directory
\begin{verbatim}
benchmark_suite/QuickSort/images/hybrid/bayeswc
\end{verbatim}
%
To view the plot, we transfer it from the Docker container to your local
filesystem.
%
Run this command in the local machine's terminal:
\begin{verbatim}
$ docker cp hybrid_aara:/home/hybrid_aara/benchmark_suite\
/QuickSort/images/hybrid/bayeswc/inferred_cost_bound.pdf \
<path-to-local-directory>
\end{verbatim}
%
You can then view the plot on your local machine.

\subsection{Generate Experimental Results}
\label{sec:generate experiment results}

This section demonstrates how to perform Hybrid AARA on the 10 benchmark
programs.
%
First go to the directory \texttt{/home/hybrid\_aara/benchmark\_suite/toolbox}.
%
Then run
\begin{verbatim}
# python3 run_experiment.py <options>
\end{verbatim}
where \texttt{<options>} specifies the benchmark name and possibly also the
analysis mode.
%
The inference result will be stored inside each benchmarks' \texttt{bin}
directory.

For example, to perform Data-Driven \Opt{} on the benchmark \texttt{QuickSort},
run
\begin{verbatim}
# # python3 run_experiment.py QuickSort data_driven opt
\end{verbatim}
%
The analysis should finish in 15 seconds.
%
To perform all data-driven resource analyses on the benchmark
\texttt{QuickSort}, run
\begin{verbatim}
# python3 run_experiment.py QuickSort data_driven
\end{verbatim}
%
It runs Data-Driven \Opt{}, Data-Driven \BayesWC{}, and Data-Driven \BayesPC{}.
%
The analysis should finish within 2 minutes.
%
If we instead want to perform all hybrid resource analyses, replace
\texttt{data\_driven} with \texttt{hybrid}:
\begin{verbatim}
# python3 run_experiment.py QuickSort hybrid
\end{verbatim}
%
It runs Hybrid \Opt{}, Hybrid \BayesWC{}, and Hybrid \BayesPC{}.
%
The analysis should finish in 11 minutes.
%
To run both purely data-driven and hybrid resource analyses on the benchmark
\texttt{QuickSort}, run
\begin{verbatim}
# python3 run_experiment.py QuickSort
\end{verbatim}
without specifying the analysis mode.

Finally, to run all 10 benchmarks (under both purely data-driven and hybrid
analyses), run
\begin{verbatim}
# python3 run_experiment.py all
\end{verbatim}
This can take up to 2 hours to run\footnote{We have tried Python's
  multiprocessing to run multiple benchmarks in parallel, but it did not work
  properly in Docker. So our artifact can only run benchmarks sequentially.}.

To view the updated tables and plots of the new experimental results, follow the
steps in \cref{sec:display experiment results}.
%
The experimental results for (Data-Driven and Hybrid) \BayesWC{} and \BayesPC{}
after rerunning the experiments may look different from the tables and plots
reported in the updated version of the paper.
%
This is because \BayesWC{} and \BayesPC{} run sampling-based Bayesian inference
algorithms that are stochastic.
%
Nonetheless, there should not be any significant deviation from the results
reported in the paper.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "README"
%%% End:
